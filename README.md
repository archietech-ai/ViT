# ViT
2010.11929v2: AN IMAGE IS WORTH 16X16 WORDS


The idea is to divide the picture into pieces like different frames or "tokens" (like tokens in LLM), we convert them to vectors, pass them through a flatten FF network with no activation function. Then we add a positional vectors to those token vectors and pass the result into encoders.  
![image](https://github.com/user-attachments/assets/7c641bfe-50d3-49d7-86e6-70aad97ebaaf)


-------------------

### Inductive Bias:

![image](https://github.com/user-attachments/assets/e4d3e51a-1875-4311-a1ff-01cf9c0e5f9a)

The assumptions we make about our data (like linearality in linear learners) is cslled Inductive Bias
More assumptions we make, more restrictive and smaller the solution domain is, so the found solution is not neccessarly the best answer. More intensive the assumptions, the number of solutions will be restrictive and sub-optimal.

In CNN we also assume some assumption like locality. When we wanted to find a feature, we use a kernel with specific size (like 3x3) and we use that to find the feature of that image. So we assume the 3x3 pixel around the current position is a good representative (but it could be 5x5 or even entire picture)

In transformers we do not have that much inductive bias. Every frame in this article can access other frames information (through encoder attention). So we do not have locality assumption here, it's more flexible. Hence less "inductive bias". We still have bias, but is less than CNN.

One message of this article is more data we have, transformers are better than CNN. When we have limited data, using more intesive inductive bias finds the solutions better. But if we have lots of data, less inductive bias result a better output.

TRansformers are the most general mochine we have created but they are very data hungry.

#### Pretraining:
ImageNet--> 1 M images and 1000 classes

ImageNet 21 K --> 14 M images and 21 K classes

JFT-303 M ---> Google owned data (not public), It has 303 M images (There is a new data set called JFT-3B too)

#### Fine Tuning:

CFAR 10 and CFAR 100

Oxford

ImageNet (validation data sets)

#### ViT variations:

Same as BERT that has base and large versions, we have differemt variations here:

![image](https://github.com/user-attachments/assets/9b457c7d-0ca5-4820-b1c8-2e75fff87dd3)


In the above table, the Hidden size D is about the number of dimentions that we are generated by Linear Projection NN in the ViT architecture:

![image](https://github.com/user-attachments/assets/b119fbfe-b663-409d-9711-58fd623707e6)



#### Performance evaluation:

The performance of the model is based on the data that is used for fine tuning. All the models in the following table use JFT-300M for pre-training.


![image](https://github.com/user-attachments/assets/33f1bca8-d18e-4cd1-b01b-6a346658361f)

BiT-L and Noisy Student are a bit older than ViL and they are CNN based. 


ViT outperform ResNet.

In the first row we see that when model is larger (meaning less inductive bias we have) the model can learn better on huge data set compare to smaller data set. ViT-H performs better than ViT-L

When we were creating patches we could create 16x16 patches (meaning we slice the picture into 16x16) where each pacth is 14x14x3.

We could also create 14x14 patches that each creates 16x16x3.

In the table where we have ViT-H/14 or ViT-L/16 it refers to patch sizes.

The last row is very important as it shows the ViL is less computationally extensive compare to older models when it comes to fine tuning. ViL-H uses 2.5 K of TPUv3-core-days while BiT-L uses 9.9 K

For exampple if we have 1000 TPU cores, Ours-I21k model needs 0:23 of a day (like 1/4 day) almost 6 hours. Compare that with BiT-L that needs almost 10 days if we have 1000 TPUs.

So when we have many many data, ViL is better while we have limited data, CNN is still better.

The following image shows if we have more data (on X axis), the accuracy (Y axis) increases. For less amount of data, BiT (gray square) has higher accuracy. Larger models have more capacity to learn. So when we give more data, bigger models have capacity to learn from it (as they do not have too much inductive bias)

![image](https://github.com/user-attachments/assets/027b7416-2aef-4d18-ae64-5979812fdfc3)


When the size of patches is decreased, the number of them are increased. That means we will have more tokens, that it leads to more complex model at the end. More token=more data--> more complex model

So decreasing the patch size is a way to increase the model quality (becuase of having more data)

### Chinchilla paper
The Chinchilla paper, officially titled "Training Compute-Optimal Large Language Models" by researchers at DeepMind, explores the relationship between model size (number of parameters) and data size (amount of training data) for training large language models efficiently. It provides critical insights into how to balance these factors to optimize model performance under a fixed compute budget.

The paper proposes that for a given compute budget, scaling both model size and training data proportionally is critical for achieving optimal performance. They argue that:

Bigger models are not always better if training data is insufficient.

Overly large models can underperform because they fail to generalize effectively when trained on limited data.
Undertraining models wastes compute.

Many large models (e.g., GPT-3) were trained with suboptimal data sizes relative to their parameter count.
The study empirically shows that training smaller models on more data can often achieve better results than training excessively large models on less data.
 

X axis is the encoder numbers
Y axis 
![image](https://github.com/user-attachments/assets/9c286cfe-53fb-4237-b405-f140b6bd4eaa)




